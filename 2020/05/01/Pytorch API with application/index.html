<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      PytorchAPI速查手册 | Backup Space 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="DFlat">
    
    

    <meta name="description" content="12import torch as tchimport numpy as np tch.where(condition, x, y) Parameters:   condition - boolTensor. x, y - Tensor where the output value selected.   Return value:   out[i] &#x3D; \begin{cases} x[i], &amp;">
<meta property="og:type" content="article">
<meta property="og:title" content="PytorchAPI速查手册 | Backup Space">
<meta property="og:url" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/index.html">
<meta property="og:site_name" content="Backup Space">
<meta property="og:description" content="12import torch as tchimport numpy as np tch.where(condition, x, y) Parameters:   condition - boolTensor. x, y - Tensor where the output value selected.   Return value:   out[i] &#x3D; \begin{cases} x[i], &amp;">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/dimension_idx.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/row_major.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/row_major_flatten.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/column_major.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/column_major_flatten.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-1_one_step_2d.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-1_one_step_1d.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-2_one_step_2d.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-2_one_step_1d.svg">
<meta property="og:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/3dTensor.svg">
<meta property="article:published_time" content="2020-05-01T13:40:54.000Z">
<meta property="article:modified_time" content="2020-06-04T16:00:17.164Z">
<meta property="article:author" content="DFlat">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="python">
<meta property="article:tag" content="ML&#x2F;DL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/05/01/Pytorch%20API%20with%20application/TensorDS/dimension_idx.svg">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    
<link rel="stylesheet" href="/css/uno.css">

    
<link rel="stylesheet" href="/css/highlight.css">

    
<link rel="stylesheet" href="/css/archive.css">

    
<link rel="stylesheet" href="/css/china-social-icon.css">


<meta name="generator" content="Hexo 4.2.1"></head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Backup Space</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">归档</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">PytorchAPI速查手册</h1>

    

    <div class="post-meta">
      <time datetime="2020-05-01" class="post-meta__date date">2020-05-01</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/ML-DL/" rel="tag">ML/DL</a>, <a class="tags-link" href="/tags/python/" rel="tag">python</a>, <a class="tags-link" href="/tags/pytorch/" rel="tag">pytorch</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> tch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h3 id="tch-where-condition-x-y"><a href="#tch-where-condition-x-y" class="headerlink" title="tch.where(condition, x, y)"></a><a href="https://pytorch.org/docs/stable/torch.html?highlight=var#torch.where" target="_blank" rel="noopener"><code>tch.where(condition, x, y)</code></a></h3><blockquote>
<p>Parameters:</p>
</blockquote>
<ul>
<li><strong>condition</strong> - boolTensor.</li>
<li><strong>x, y</strong> - Tensor where the output value selected.</li>
</ul>
<blockquote>
<p>Return value:</p>
</blockquote>
<script type="math/tex; mode=display">
out[i] = \begin{cases}
x[i], & condition[i] \hspace{3pt} is \hspace{3pt} True \\
y[i], & \hspace{35pt}else
\end{cases}</script><blockquote>
<p>Application</p>
</blockquote>
<ul>
<li><p>Hard one-hot Encoding for Image Segmentation</p>
<p>假设原始标注mask灰度值范围在[0, 255], 若灰度值非零则置1, 否则不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bMask = tch.where(mask &gt; <span class="number">0</span>, torch.ones(mask.shape), mask)</span><br><span class="line"><span class="keyword">return</span> bMask</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="tch-var-input-dim-keepdim-False-unbiased-True-out-None"><a href="#tch-var-input-dim-keepdim-False-unbiased-True-out-None" class="headerlink" title="tch.var(input, dim, keepdim=False, unbiased=True, out=None)"></a><a href="https://pytorch.org/docs/stable/torch.html?highlight=var#torch.var" target="_blank" rel="noopener"><code>tch.var(input, dim, keepdim=False, unbiased=True, out=None)</code></a></h3><blockquote>
<p>Parameters:</p>
</blockquote>
<ul>
<li><strong>input</strong> - input tensor.  </li>
<li><strong>dim</strong> - compute variance along <code>dim</code> dimension.</li>
<li><strong>keepdim</strong> - whether extra <code>tch.squeeze()</code> or not.</li>
<li><strong>unbiased</strong> - whether Bessel’s correction.</li>
</ul>
<blockquote>
<p>Application</p>
</blockquote>
<ul>
<li><code>BatchNorm1d</code>的计算</li>
<li><p><code>BatchNorm2d</code>的计算  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean = tch.mean(feature)</span><br><span class="line">std = tch.var(feature, unbiased=<span class="literal">False</span>).sqrt()</span><br><span class="line">normed_feature = (feature - mean) / std</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="tch-tensor-data-dtype-None-device-None-requires-grad-False-pin-memory-False-vs-tch-Tensor"><a href="#tch-tensor-data-dtype-None-device-None-requires-grad-False-pin-memory-False-vs-tch-Tensor" class="headerlink" title="tch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) vs tch.Tensor()"></a><a href="https://pytorch.org/docs/stable/tensors.html?highlight=tensor#torch.Tensor" target="_blank" rel="noopener"><code>tch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)</code></a> vs <a href="https://pytorch.org/docs/stable/tensors.html?highlight=tensor#torch.Tensor" target="_blank" rel="noopener"><code>tch.Tensor()</code></a></h3><blockquote>
<p>二者异同：均可定义Tensor, 但：</p>
</blockquote>
<ul>
<li><p>tch.tensor()是函数. 该方式比较灵活，推荐这种方式定义.</p>
</li>
<li><p>tch.Tensor是类，默认数据类型<code>float32</code>. 初始化时<strong>不能指定dtype</strong>.</p>
</li>
</ul>
<blockquote>
<p>Parameters for <code>tch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)</code></p>
</blockquote>
<ul>
<li><strong>device</strong> - object of <code>tch.device</code>, indicate the device that the tensor will be allocated to.</li>
</ul>
<blockquote>
<p>Example:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tch.tensor(data, dtype=tch.float32, device=tch.device(<span class="string">"cpu"</span>))</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="tch-device"><a href="#tch-device" class="headerlink" title="tch.device"></a><a href="https://pytorch.org/docs/stable/tensor_attributes.html?highlight=torch%20device#torch.torch.device" target="_blank" rel="noopener"><code>tch.device</code></a></h3><blockquote>
<p>Python class, parameters for <code>__init__()</code>:</p>
</blockquote>
<p>&emsp; 重载一：</p>
<ul>
<li><strong>device</strong> - str, device with ordinal. e.g <code>&quot;cuda:0&quot;</code></li>
</ul>
<p>&emsp; 重载二：</p>
<ul>
<li><strong>type</strong> - str, device type. e.g. <code>&quot;cuda&quot;</code> or <code>&quot;cpu&quot;</code></li>
<li><strong>index</strong> - device ordinal.</li>
</ul>
<blockquote>
<p>Example:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpu_0 = tch.device(<span class="string">"cuda:0"</span>)</span><br><span class="line">gpu_1 = tch.device(<span class="string">"cuda"</span>, <span class="number">1</span>)</span><br><span class="line">cpu_0 = tch.device(<span class="string">"cpu"</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h3><blockquote>
<p>两眼一抹黑的Basic usage</p>
</blockquote>
<ul>
<li>存断点：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tch.save(model.state_dict(), checkpoint_file_path)</span><br></pre></td></tr></table></figure>
<ul>
<li>写好的模型直接加载参数，套娃即可：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载参数到cpu</span></span><br><span class="line">model.load_state_dict(tch.load(checkpoint_file_path), \</span><br><span class="line">						map_location=tch.device(<span class="string">"cpu"</span>))</span><br><span class="line"><span class="comment"># 加载参数到gpu</span></span><br><span class="line">model.load_state_dict(tch.load(checkpoint_file_path), \</span><br><span class="line">						map_location=tch.device(<span class="string">"cuda:0"</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>加载模型 + 参数：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = tch.load(model_file)</span><br></pre></td></tr></table></figure>
<h3 id="Save-tch-save-obj-f"><a href="#Save-tch-save-obj-f" class="headerlink" title="Save: tch.save(obj, f, ...)"></a>Save: <a href="https://pytorch.org/docs/stable/torch.html?highlight=load#torch.save" target="_blank" rel="noopener"><code>tch.save(obj, f, ...)</code></a></h3><blockquote>
<p>Parameters:</p>
</blockquote>
<ul>
<li><strong>obj</strong> - the object to be saved.</li>
<li><strong>f</strong> - a string specifying the path of output checkpoint file, or a file-like object (has to implement <code>write()</code> and <code>flush()</code>).<br>其余参数略复杂且不常用，略去。</li>
</ul>
<h3 id="Load-checkpoint-file-tch-load-f-map-location-None"><a href="#Load-checkpoint-file-tch-load-f-map-location-None" class="headerlink" title="Load checkpoint file: tch.load(f, map_location=None, ...)"></a>Load checkpoint file: <a href="https://pytorch.org/docs/stable/torch.html?highlight=load#torch.load" target="_blank" rel="noopener"><code>tch.load(f, map_location=None, ...)</code></a></h3><p>从断点文件加载带参模型 或 保存的模型参数。</p>
<blockquote>
<p>Parameters:</p>
</blockquote>
<ul>
<li><strong>f</strong> - a string that specify the path of checkpoint file, or a file-like object (has to implement <code>read()</code>, <code>readline()</code>, <code>tell()</code>, and <code>seek()</code>).</li>
<li><strong>map_location</strong> - a function, <code>torch.device</code> object, string or a dict specifying how to remap storage locations. (直接引自官网，这个参数解释的不能更清楚了).<br>其余参数略复杂且不常用，略去。</li>
</ul>
<h3 id="Load-to-model-model-load-state-dict-state-dict-strict-True"><a href="#Load-to-model-model-load-state-dict-state-dict-strict-True" class="headerlink" title="Load to model: model.load_state_dict(state_dict, strict=True)"></a>Load to model: <a href="https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict" target="_blank" rel="noopener"><code>model.load_state_dict(state_dict, strict=True)</code></a></h3><p>将保存的模型参数赋值给模型。</p>
<blockquote>
<p>Parameters:</p>
</blockquote>
<ul>
<li><strong>state_dict</strong> - a dict containing parameters and persistent buffers.</li>
<li><strong>strict</strong> - if <code>True</code>, the keys of <strong><code>state_dict</code></strong> must exactly match the keys returned by model’s <code>state_dict()</code>.</li>
</ul>
<h3 id="tch-nonzero-input-as-tuple-False"><a href="#tch-nonzero-input-as-tuple-False" class="headerlink" title="tch.nonzero(input, as_tuple=False)"></a><a href="https://pytorch.org/docs/stable/torch.html?highlight=diag#torch.nonzero" target="_blank" rel="noopener"><code>tch.nonzero(input, as_tuple=False)</code></a></h3><p>返回Tensor中非零元素的索引，索引升序排列。该方法可用于筛选符合条件的元素或通道(切片)。</p>
<blockquote>
<p>Parameters:</p>
</blockquote>
<ul>
<li>input: Tensor to be operated.</li>
<li>as_tuple: Whether split every index dimension of non-zero value into a tuple. Default False.</li>
</ul>
<blockquote>
<p>Example:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat = mat=tch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>],</span><br><span class="line">                          [<span class="number">0</span>,<span class="number">1</span>,<span class="number">9</span>],</span><br><span class="line">                          [<span class="number">0</span>,<span class="number">0</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(mat, as_tuple=<span class="literal">True</span>) \</span><br><span class="line">    <span class="comment"># 等价于 mat.nonzero(as_tuple=True)</span></span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]), tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<p>如上demo所示，参数<code>as_tuple=True</code>, 非零元素的索引[0, 0], [0, 1], [1, 1], [1, 2], [2, 2]的两个维度被分解到tuple中的两项，所有索引第一个维度的值组成tensor([0, 1, 2, 3]), 作为返回tuple的第一项。同理，所有索引第二维组成tensor([0, 1, 2, 3])作为返回tuple的第二项。</p>
<blockquote>
<p>Application</p>
</blockquote>
<ul>
<li>图像 or mask标签中的全零元素通道去除（避免负样本淹没）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">non_zero_ch_idx = img.nonzero(as_tuple=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">non_zero_ch_idx = non_zero_channel.unique().cpu()</span><br><span class="line"><span class="comment"># 假定非零通道索引连续</span></span><br><span class="line">non_zero_img = img[non_zero_ch_idx[<span class="number">0</span>]:non_zero_ch_idx[<span class="number">-1</span>] + <span class="number">1</span>]</span><br><span class="line"><span class="keyword">return</span> non_zero_img</span><br></pre></td></tr></table></figure>
<ul>
<li>进一步：筛选符合某条件的元素的索引</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 返回元素值大于1的索引 """</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat = tch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">9</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">0</span>, <span class="number">-1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">9</span>],</span><br><span class="line">        [ <span class="number">0</span>,  <span class="number">0</span>, <span class="number">-1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(mat &gt; <span class="number">1</span>).nonzero()</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<ul>
<li>再进一步：筛选符合条件的图像通道</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 筛选最大值大于零的图像通道，返回满足条件的通道索引。</span></span><br><span class="line"><span class="string">    设图像为3维Tensor</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>img</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">         [ <span class="number">0</span>, <span class="number">-1</span>, <span class="number">-9</span>],</span><br><span class="line">         [ <span class="number">0</span>,  <span class="number">0</span>, <span class="number">-1</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">10</span>, <span class="number">19</span>],</span><br><span class="line">         [ <span class="number">5</span>,  <span class="number">8</span>,  <span class="number">0</span>],</span><br><span class="line">         [ <span class="number">2</span>,  <span class="number">1</span>,  <span class="number">3</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>img = img.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>)  <span class="comment"># [C, H, W] → [C, HW]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>img.max(axis=<span class="number">1</span>)</span><br><span class="line">torch.return_types.max(</span><br><span class="line">values=tensor([ <span class="number">0</span>, <span class="number">19</span>]),</span><br><span class="line">indices=tensor([<span class="number">7</span>, <span class="number">2</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(img.max(axis=<span class="number">1</span>)[<span class="number">0</span>] &gt; <span class="number">0</span>).nonzero(as_tuple=<span class="literal">True</span>)</span><br><span class="line">(tensor([<span class="number">1</span>]),)  <span class="comment"># 通道0所有元素非正，丢弃。通道1符合条件，返回通道索引。</span></span><br></pre></td></tr></table></figure>
<h3 id="Tensor维度交换与变形"><a href="#Tensor维度交换与变形" class="headerlink" title="Tensor维度交换与变形"></a>Tensor维度交换与变形</h3><blockquote>
<p>概述：Tensor数据结构的组织</p>
</blockquote>
<p>无论多少维的Tensor，其底层实现均为使用一段连续内存的一维数组（而非多维数组，尽管多维数组同样使用连续内存）。除这个一维数组外，Tensor还额外维护一些属性，这些属性控制Tensor的形状和元素的访问，这些属性称为<code>元信息</code>。元信息内的<code>stride</code>和<code>shape</code>两个属性联合控制数据读取的维度。访问元素时，以<code>起始地址+偏移量</code>的方式访问Tensor的元素，其中起始地址为一维数组的起始地址（i.e. 一维数组第一个元素的地址），偏移量由<code>stride</code>和<code>shape</code>联合计算。</p>
<blockquote>
<p>Tensor元素的访存</p>
</blockquote>
<ul>
<li><p>Tensor在底层为什么使用一维数组？</p>
<p>计组告诉我们，内存是一维的线性结构，寻址方式为<code>起始地址+偏移量</code>。通过这种方式，内存可以实现非常高效的随机存取。模拟内存的这种行为，我们就能处理任意维度的张量。为此需要两个必须要素：起始地址和偏移量。起始地址是容易得到的：一维数组的首地址。而偏移量需要根据索引进行计算，这需要额外维护<code>shape</code>和<code>stride</code>两个属性：<code>shape</code>指向一个元组，元组内存储Tensor各维的容量（长度），<code>stride</code>指向另一个元组，元组内存储访问一维数组数据的步长。举例来说：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat = tch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="comment"># 底层存储为一维数组[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]</span></span><br><span class="line"><span class="comment"># 我们暂设这个一维数组为m</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat.stride()</span><br><span class="line">(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># stride[1] = 1意味着：</span></span><br><span class="line"><span class="comment"># mat[0,0] = m[0], mat[0,1] = m[1], mat[0,2] = m[2]</span></span><br><span class="line"><span class="comment"># stride[0] = 3意味着：</span></span><br><span class="line"><span class="comment"># mat[0,0] = m[0*3+0], mat[1,0] = m[1*3+0]</span></span><br><span class="line"><span class="comment"># mat[1,1] = m[1*3+1], mat[1,2] = m[1*3+2]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat.shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>])  <span class="comment"># 3x4 matrix</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>重要问题：<code>stride</code>是什么，怎么算？</p>
<p><code>stride[i]</code>定义为：某维度下，相邻元素的间隔<sup><a href="#fn_margin" id="reffn_margin">margin</a></sup>。即：第$i$维索引增加1(称为自增)而其他维索引不变时，在底层一维数组上，自增后元素位置（地址）相对于自增前元素位置（地址）的偏移量。<br>对于一个N维的Tensor，在<code>行优先</code>且Tensor<code>连续</code>（参见下一小节）的条件下：</p>
<script type="math/tex; mode=display">
stride[i] = \begin{cases}
\hspace{56pt}1, && i=N-1 \\
stride[i+1] \times shape[i+1], && i=0,1,...,N-2
\end{cases} \tag{0.a}</script><p>其中，$i$为维度的索引，最高维索引$i=0$，最低维索引$i=N$。因此$stride[i]$表示第$i$维的stride:<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/dimension_idx.svg" alt><br>几张图解释相关概念和公式：<br>行优先存储(C/C++):<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/row_major.svg" alt="row-major" title="行优先的二维矩阵"><br>用<code>mat</code>表示此矩阵。<br>行优先的一维数组：<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/row_major_flatten.svg" alt="row-major_in_memory" title="行优先的底层/内存表示"><br>用<code>array</code>表示此一维数组。<br>列优先存储(MATLAB, Fortran):<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/column_major.svg" alt="column-major" title="列优先的二维矩阵"><br>列优先的一维数组：<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/column_major_flatten.svg" alt="column-major_in_memory" title="列优先的底层/内存表示"><br>pytorch的底层是C++, 行优先下的stride:<br><code>mat[0,0] → mat[0,1]</code>对应<code>array[0*4+0] → array[0*4+1]</code>, stride = 1:<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-1_one_step_2d.svg" alt><br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-1_one_step_1d.svg" alt><br><code>mat[0,0] → mat[1,0]</code>对应<code>array[0*4+0] → array[1*4+0]</code>, stride = 4:<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-2_one_step_2d.svg" alt><br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/axis_-2_one_step_1d.svg" alt><br>更粗暴一点，可以将上面的递归公式$(0.\text{a})$解出来，变为循环的方式：</p>
<script type="math/tex; mode=display">
stride[i]=\begin{cases}
\hspace{32pt}1, && i=N-1 \\
\prod_{k=i+1}^{N-1} shape[k], && i=0,1,...,N-2
\end{cases} \tag{0.b}</script><p>一言以蔽之：第$i$维的stride等于shape从$i+1$索引到$N-1$，所有值的乘积。</p>
</li>
<li><p>重要问题：如何用将多维索引转化为一维索引？</p>
<script type="math/tex; mode=display">
index\_1d = index\_nd \cdot stride =\sum_{\forall i} index\_nd[i] \times stride[i] \tag{1}</script><p>上式中，$index_1d$为转化后的一维索引。$index_nd$和$index_nd[i]$分别为多维索引向量与其第$i$维的值。$stride$和$stride[i]$分别为<code>stride</code>向量与其第$i$维的值。<strong>简言之，就是索引向量和stride向量的内积</strong>。举一个3D张量的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat = tch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat</span><br><span class="line">tensor([[[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">         [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat.shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat.stride()</span><br><span class="line">(<span class="number">6</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data=mat.flatten()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]</span><br><span class="line">tensor(<span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>idx = np.dot([<span class="number">6</span>,<span class="number">3</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>idx</span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[idx]</span><br><span class="line">tensor(<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以换个方向读数据——维度交换：<code>transpose()</code>与<code>permute()</code></p>
<p>一般情况下，我们读取二维数据的习惯是：从左上第一个元素开始，从左到右，从上到下，即行优先。索引语法上表现为：从左到右自增的优先级递增，即：[0][0]→[0][1]→[1][0]→[1][1]→…以此类推。<br>现在来想想我们是如何进行二维矩阵的转置的。转置前我们按照默认方式读数据：</p>
<script type="math/tex; mode=display">
[0][0]→[0][1]→[1][0]→[1][1]→... \tag{2}</script><p>转置时，我们只不过是按列优先方式读数据：</p>
<script type="math/tex; mode=display">
[0][0]→[1][0]→[0][1]→[1][1]→... \tag{3}</script><p>明显能看出，我们只是改变了优先遍历哪个维度。从编程语言和框架使用者的角度考虑，我们希望索引语法是一致的，也就是说转置前后的tensor都能用$(2)$的语法访问到正确的元素。<br>另一方面，我们拥有三个强有力的工具：一维数组，元信息内的<code>shape</code>和<code>stride</code>属性，以及公式$(1)$。对于给定的多维索引向量，公式$(1)$将其与一维数组和元信息结合起来，极灵活地访问元素（即随机存取）。这意味着对于诸如转置这样的维度交换工作，我们完全可以不实际交换底层数据的顺序，而仅改变读数据的方式。这一点恰好与上面改变维度遍历的优先级的想法是一致的。<br>举一个3维的例子：<br><img src="/2020/05/01/Pytorch%20API%20with%20application/TensorDS/3dTensor.svg" alt="3dTensor"><br>上图标识了两种可能的读数据的方向：紫+蓝是默认的读数据方向，即先x(W), 后y(H), 最后z(C or D)，为简单起见，记此维度优先级为(C, H, W)。黄+红是另一种合理的读数据方向，维度优先级为(H, W, C)。根据前面的分析，<u>我们希望将这种<strong>视角的改变</strong>与<strong>改变维度遍历的优先级</strong>这两件事统一起来</u>。<br>综上，我们希望基于以下前提完成转置工作：  </p>
<ol>
<li>保持索引语法的一致性。</li>
<li>利用元信息控制元素访问，而不实际改变底层数据。</li>
</ol>
<p>为达到这个目的，只需要一个简单的证明：<br>设矩阵$A_{m \times n}$，为不失一般性，设其<code>stride</code>为$(s_0,s_1)$。考虑任意一个确定的元素$A[i][j]$：根据上一小节的索引转换公式$(1)$，该元素在一维数组上的索引为：</p>
<script type="math/tex; mode=display">idx\_1d = (i,j) \cdot (s_0,s_1) = i*s_0+j*s_1</script><p>转置后，该元素在转置矩阵$A^T$上的位置（索引）就变为了$[j][i]$，对转置矩阵使用新索引$[j][i]$才能访问到该元素。但另一方面，该元素在底层一维数组上的位置（索引）并未改变，也就是说上式计算出的$idx_1d$的值是不变的。因此，在转置矩阵上使用新索引$[j][i]$访问$A^T[j][i]$时：</p>
<script type="math/tex; mode=display">
idx\_1d = (j,i) \cdot (s_1,s_0) = j*s_1+i*s_0</script><p>这说明，新的转置矩阵的<code>stride</code>是通过交换原矩阵<code>stride</code>元组的指定维得到的，$i.e. \hspace{2pt} (s_0,s_1) \rightarrow (s_1, s_0).$<br>另外，由于变换了优先读数据的方向（维度优先级），那么各维的数据边界相应地发生了变化，因此<code>shape</code>元组的值也应当相应进行交换：$(m,n)\rightarrow(n,m).$<br>容易推广到一般情况：多个维度的交换，都可以看作若干次“转置”操作。因此对于维度的任意排列组合，只需对<code>stride</code>和<code>shape</code>做相同的排列组合即可。<br>在pytorch中，维度交换的操作主要由两个函数(方法)完成：任意<strong>两维</strong>的交换由<code>transpose()</code>完成，任意<strong>多维</strong>交换（维度的排列组合）由<code>permute()</code>完成。多数ML/DL框架利用不修改底层数据的特性，维度交换后的tensor仅创建一份新的元信息，与原始tensor共享同一份底层数据。<br>到此为止，我们拥有的处理tensor访问的强有力工具：</p>
<ul>
<li><strong>一维数组</strong></li>
<li><strong>元信息的<code>stride</code>和<code>shape</code></strong></li>
<li><strong>公式</strong>$(1)$</li>
</ul>
<p>可以灵活地进行元素访问和维度交换。</p>
</li>
</ul>
<blockquote>
<p>Tensor的连续性</p>
</blockquote>
<ul>
<li><p>Tensor连续的定义</p>
<p>直观定义：若Tensor元素的语义顺序（即按<code>stride</code>和<code>shape</code>一维展开）与底层一维数组顺序（内存存储顺序）一致，则是连续的，反之非连续。一维展开是按行优先还是列优先，视框架和编程语言决定。例如pytorch，其底层为C++，则是按行优先展开。<br>数学上的定义：Tensor是连续的，当且仅当<code>stride</code>和<code>shape</code>满足关系：</p>
<script type="math/tex; mode=display">
stride[i] = stride[i+1] \times shape[i+1]</script></li>
<li><p>何时&amp;为何需要tensor连续？</p>
<ol>
<li>tensor的view()操作要求tensor是连续的，参考下一小节。</li>
<li>连续的tensor可以利用cache加速。<br>在内存中存储连续的数据，命中缓存的概率高。访问相邻元素是矩阵运算的高频操作（例如卷积），因此连续tensor可以被缓存加速。</li>
</ol>
</li>
</ul>
<blockquote>
<p>变形操作要求tensor连续</p>
</blockquote>
<p>  利用我们持有的三个工具：一维数组，元信息以及公式$(1)$，我们可以灵活地对元素随机存取。那么可以想见，tensor的变形操作也可以利用元信息的变换来完成。<br>  所谓变形，仍然是改变了读数据的方式，不过这次的改变相比起维度交换更加具一般性且更复杂。为了说明这个问题，考虑一个例子：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat   <span class="comment"># tensor创建代码省略</span></span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat.is_contiguous()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat.stride()</span><br><span class="line">(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat_trans = mat.transpose(<span class="number">1</span>,<span class="number">0</span>)  <span class="comment"># 维度交换影响连续性</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat_trans</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">5</span>,  <span class="number">9</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">6</span>, <span class="number">10</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">7</span>, <span class="number">11</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">8</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat_trans.stride()</span><br><span class="line">(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mat_trans.is_contiguous()</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" target="_blank" rel="noopener"><code>tch.Tensor.view()</code></a><br>view要求tensor是连续或被view的子空间是连续的（子维度连续）。<br>view操作涉及pytorch中的很多函数和方法，是一个非常大的主题。比较详细的官方指引可以参考：<a href="https://pytorch.org/docs/stable/tensor_view.html" target="_blank" rel="noopener">TENSOR VIEWS</a>. 在实践中，不妨采取工程的最佳实践：使用并迭代地丰富自己的经验。</li>
<li><a href="https://pytorch.org/docs/stable/torch.html#torch.reshape" target="_blank" rel="noopener"><code>tch.reshape()</code></a>  </li>
</ul>
<blockquote id="fn_margin">
<sup>margin</sup>. 在ML/DL框架中，间隔一般是指逻辑间隔而非物理间隔，即：以单个元素(数据)为单位，而非bit或byte.<a href="#reffn_margin" title="Jump back to footnote [margin] in the text."> &#8617;</a>
</blockquote>

  </section>

  <section class="post-comments">

    <!-- 将评论系统（例如Disqus、多说、友言、畅言等）提供的代码片段粘贴在这里 -->
    
</section>


</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno" target="_blank" rel="noopener">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/jquery.min.js"></script>
    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
